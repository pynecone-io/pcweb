from flexdown.modules._ead951883d06d62f020c58130ddcbdefa1932716071ada2cb81e96b0dbffc4d1 import *

import reflex as rx
from openai import OpenAI
from reflex_chat import chat

try:
    client = OpenAI()
except:
    client = None

# Only define your logic, the chat component handles the rest.
async def run_llm(chat_state):
    # Start a new session to answer the question.
    session = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=chat_state.get_messages(),
        stream=True,
    )

    # Stream the results, yielding after every word.
    for item in session:
        if hasattr(item.choices[0].delta, "content"):
            answer_text = item.choices[0].delta.content
            # Ensure answer_text is not None before concatenation
            chat_state.append_to_response(answer_text)
            yield
